{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will write a matrix factorization model in pytorch to solve a recommendation problem. Then we will write a more general neural model for the same problem.\n",
    "\n",
    "Collaborative filtering: systems recommend items based on similarity measures between users and/or items. The items recommended to a user are those preferred by similar users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MovieLens dataset (ml-latest-small) describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 100004 ratings and 1296 tag applications across 9125 movies. https://grouplens.org/datasets/movielens/. To get the data:\n",
    "\n",
    "`wget http://files.grouplens.org/datasets/movielens/ml-latest-small.zip`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MovieLens dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/yinterian/teaching/ML-2/data/ml-latest-small/links.csv'),\n",
       " PosixPath('/Users/yinterian/teaching/ML-2/data/ml-latest-small/tags.csv'),\n",
       " PosixPath('/Users/yinterian/teaching/ML-2/data/ml-latest-small/ratings.csv'),\n",
       " PosixPath('/Users/yinterian/teaching/ML-2/data/ml-latest-small/README.txt'),\n",
       " PosixPath('/Users/yinterian/teaching/ML-2/data/ml-latest-small/movies.csv')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = Path(\"/Users/yinterian/teaching/ML-2/data/ml-latest-small/\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId,movieId,rating,timestamp\r",
      "\r\n",
      "1,31,2.5,1260759144\r",
      "\r\n",
      "1,1029,3.0,1260759179\r",
      "\r\n",
      "1,1061,3.0,1260759182\r",
      "\r\n",
      "1,1129,2.0,1260759185\r",
      "\r\n",
      "1,1172,4.0,1260759205\r",
      "\r\n",
      "1,1263,2.0,1260759151\r",
      "\r\n",
      "1,1287,2.0,1260759187\r",
      "\r\n",
      "1,1293,2.0,1260759148\r",
      "\r\n",
      "1,1339,3.5,1260759125\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! head /Users/yinterian/teaching/deeplearning/data/ml-latest-small/ratings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading a csv into pandas\n",
    "data = pd.read_csv(PATH/\"ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1260759144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1029</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1260759179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1061</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1260759182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1129</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1260759185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1172</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1260759205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0       1       31     2.5  1260759144\n",
       "1       1     1029     3.0  1260759179\n",
       "2       1     1061     3.0  1260759182\n",
       "3       1     1129     2.0  1260759185\n",
       "4       1     1172     4.0  1260759205"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding data\n",
    "We enconde the data to have contiguous ids for users and movies. You can think about this as a categorical encoding of our two categorical variables userId and movieId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and validation before encoding\n",
    "np.random.seed(3)\n",
    "msk = np.random.rand(len(data)) < 0.8\n",
    "train = data[msk].copy()\n",
    "val = data[~msk].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is a handy function modified from fast.ai\n",
    "def proc_col(col, train_col=None):\n",
    "    \"\"\"Encodes a pandas column with continous ids. \n",
    "    \"\"\"\n",
    "    if train_col is not None:\n",
    "        uniq = train_col.unique()\n",
    "    else:\n",
    "        uniq = col.unique()\n",
    "    name2idx = {o:i for i,o in enumerate(uniq)}\n",
    "    return name2idx, np.array([name2idx.get(x, -1) for x in col]), len(uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(df, train=None):\n",
    "    \"\"\" Encodes rating data with continous user and movie ids. \n",
    "    If train is provided, encodes df with the same encoding as train.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for col_name in [\"userId\", \"movieId\"]:\n",
    "        train_col = None\n",
    "        if train is not None:\n",
    "            train_col = train[col_name]\n",
    "        _,col,_ = proc_col(df[col_name], train_col)\n",
    "        df[col_name] = col\n",
    "        df = df[df[col_name] >= 0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    userId  movieId  rating\n",
       "0        0        0       4\n",
       "1        0        1       5\n",
       "2        1        1       5\n",
       "3        1        2       3\n",
       "4        2        0       4\n",
       "5        2        1       4\n",
       "6        3        0       5\n",
       "7        3        3       2\n",
       "8        4        0       1\n",
       "9        4        3       4\n",
       "10       5        3       5\n",
       "11       6        1       1\n",
       "12       6        3       3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check my implementation\n",
    "df_t = pd.read_csv(\"test_data/tiny_training2.csv\")\n",
    "df_v = pd.read_csv(\"test_data/tiny_val2.csv\")\n",
    "df_t_e = encode_data(df_t)\n",
    "df_v_e = encode_data(df_v, df_t)\n",
    "df_v_e\n",
    "df_t_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = encode_data(train)\n",
    "df_val = encode_data(val, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.1881, -0.8265,  0.0338],\n",
       "        [-0.5068, -1.1261, -0.5361],\n",
       "        [-0.5703, -0.0012, -0.1918],\n",
       "        [-0.2228,  0.4780, -0.5000],\n",
       "        [ 0.0603, -1.2193, -0.3168],\n",
       "        [ 0.6763,  0.7838, -0.7186],\n",
       "        [-1.3731, -0.1078, -0.3492],\n",
       "        [-0.2062, -0.5206,  1.0072],\n",
       "        [ 1.7192, -0.7729, -0.2444],\n",
       "        [-0.7064,  0.2915,  0.5784]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an Embedding module containing 10 users or items embedding size 3\n",
    "# embedding will be initialized at random\n",
    "embed = nn.Embedding(10, 3)\n",
    "embed.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5068, -1.1261, -0.5361],\n",
       "         [-1.1881, -0.8265,  0.0338],\n",
       "         [-0.5068, -1.1261, -0.5361],\n",
       "         [ 0.0603, -1.2193, -0.3168],\n",
       "         [ 0.6763,  0.7838, -0.7186],\n",
       "         [-0.5068, -1.1261, -0.5361]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# given a list of ids we can \"look up\" the embedding corresponing to each id\n",
    "# can you see that some vectors are the same?\n",
    "a = torch.LongTensor([[1,0,1,4,5,1]])\n",
    "embed(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix factorization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size=100):\n",
    "        super(MF, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        # initlializing weights\n",
    "        self.user_emb.weight.data.uniform_(0,0.05)\n",
    "        self.item_emb.weight.data.uniform_(0,0.05)\n",
    "        \n",
    "    def forward(self, u, v):\n",
    "        u = self.user_emb(u)\n",
    "        v = self.item_emb(v)\n",
    "        return (u*v).sum(1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging MF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    userId  movieId  rating\n",
       "0        0        0       4\n",
       "1        0        1       5\n",
       "2        1        1       5\n",
       "3        1        2       3\n",
       "4        2        0       4\n",
       "5        2        1       4\n",
       "6        3        0       5\n",
       "7        3        3       2\n",
       "8        4        0       1\n",
       "9        4        3       4\n",
       "10       5        3       5\n",
       "11       6        1       1\n",
       "12       6        3       3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = 7\n",
    "num_items = 4\n",
    "emb_size = 3\n",
    "\n",
    "user_emb = nn.Embedding(num_users, emb_size)\n",
    "item_emb = nn.Embedding(num_items, emb_size)\n",
    "users = torch.LongTensor(df_t_e.userId.values)\n",
    "items = torch.LongTensor(df_t_e.movieId.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = user_emb(users)\n",
    "V = item_emb(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2874, -0.4427, -0.7411],\n",
       "        [ 1.2874, -0.4427, -0.7411],\n",
       "        [-0.9783,  0.5827,  1.0137],\n",
       "        [-0.9783,  0.5827,  1.0137],\n",
       "        [ 1.0557, -0.4477, -0.4696],\n",
       "        [ 1.0557, -0.4477, -0.4696],\n",
       "        [-1.2368,  0.6748, -0.6885],\n",
       "        [-1.2368,  0.6748, -0.6885],\n",
       "        [-0.9106,  0.6204, -0.1956],\n",
       "        [-0.9106,  0.6204, -0.1956],\n",
       "        [ 0.2215,  0.4014, -0.1028],\n",
       "        [ 0.0061,  0.4803,  0.0214],\n",
       "        [ 0.0061,  0.4803,  0.0214]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9865, -0.4910,  0.7336],\n",
       "        [ 0.2174, -0.4591, -0.5186],\n",
       "        [-0.1652,  0.6042,  0.7093],\n",
       "        [-1.0072, -0.7783, -0.2334],\n",
       "        [-0.8090, -0.4965,  0.4648],\n",
       "        [ 0.1783, -0.4642, -0.3286],\n",
       "        [ 0.9478,  0.7484,  0.6815],\n",
       "        [ 1.3673, -0.3748, -0.7869],\n",
       "        [ 0.6978,  0.6880,  0.1936],\n",
       "        [ 1.0066, -0.3446, -0.2236],\n",
       "        [-0.2449, -0.2230, -0.1175],\n",
       "        [ 0.0010,  0.4980,  0.0150],\n",
       "        [-0.0067, -0.2667,  0.0244]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# element wise multiplication\n",
    "U*V "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7440, -0.7602,  1.1483, -2.0188, -0.8407, -0.6145,  2.3776,\n",
       "         0.2056,  1.5795,  0.4385, -0.5853,  0.5139, -0.2490])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what we want is a dot product per row\n",
    "(U*V).sum(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training MF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "671 8442\n"
     ]
    }
   ],
   "source": [
    "num_users = len(df_train.userId.unique())\n",
    "num_items = len(df_train.movieId.unique())\n",
    "print(num_users, num_items) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are not using data loaders because our data fits well in memory\n",
    "def train_epocs(model, epochs=10, lr=0.01, wd=0.0, unsqueeze=False):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        users = torch.LongTensor(df_train.userId.values)  #.cuda()\n",
    "        items = torch.LongTensor(df_train.movieId.values) #.cuda()\n",
    "        ratings = torch.FloatTensor(df_train.rating.values)  #.cuda()\n",
    "        if unsqueeze:\n",
    "            ratings = ratings.unsqueeze(1)\n",
    "        y_hat = model(users, items)\n",
    "        loss = F.mse_loss(y_hat, ratings)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        testloss = valid_loss(model, unsqueeze)\n",
    "        print(\"train loss %.3f valid loss %.3f\" % (loss.item(), testloss)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([79799])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([79799, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is what unsqueeze does\n",
    "ratings = torch.FloatTensor(df_train.rating.values)\n",
    "print(ratings.shape)\n",
    "ratings = ratings.unsqueeze(1) #.cuda()\n",
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_loss(model, unsqueeze=False):\n",
    "    model.eval()\n",
    "    users = torch.LongTensor(df_val.userId.values) # .cuda()\n",
    "    items = torch.LongTensor(df_val.movieId.values) #.cuda()\n",
    "    ratings = torch.FloatTensor(df_val.rating.values) #.cuda()\n",
    "    if unsqueeze:\n",
    "        ratings = ratings.unsqueeze(1)\n",
    "    y_hat = model(users, items)\n",
    "    loss = F.mse_loss(y_hat, ratings)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MF(num_users, num_items, emb_size=100)  # if you have a GPU .cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 13.235 valid loss 5.204\n",
      "train loss 5.145 valid loss 2.340\n",
      "train loss 2.338 valid loss 3.533\n",
      "train loss 3.336 valid loss 1.137\n",
      "train loss 0.902 valid loss 2.098\n",
      "train loss 1.854 valid loss 3.024\n",
      "train loss 2.786 valid loss 2.531\n",
      "train loss 2.293 valid loss 1.393\n",
      "train loss 1.152 valid loss 1.133\n",
      "train loss 0.903 valid loss 1.858\n",
      "train loss 1.661 valid loss 1.633\n",
      "train loss 1.486 valid loss 0.934\n",
      "train loss 0.828 valid loss 1.005\n",
      "train loss 0.906 valid loss 1.451\n",
      "train loss 1.333 valid loss 1.542\n",
      "train loss 1.396 valid loss 1.205\n",
      "train loss 1.030 valid loss 0.907\n",
      "train loss 0.700 valid loss 1.058\n",
      "train loss 0.823 valid loss 1.302\n",
      "train loss 1.050 valid loss 1.134\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=20, lr=0.1, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.880 valid loss 0.894\n",
      "train loss 0.648 valid loss 0.880\n",
      "train loss 0.645 valid loss 0.915\n",
      "train loss 0.690 valid loss 0.909\n",
      "train loss 0.693 valid loss 0.872\n",
      "train loss 0.662 valid loss 0.836\n",
      "train loss 0.628 valid loss 0.819\n",
      "train loss 0.610 valid loss 0.823\n",
      "train loss 0.608 valid loss 0.834\n",
      "train loss 0.609 valid loss 0.839\n",
      "train loss 0.604 valid loss 0.836\n",
      "train loss 0.591 valid loss 0.833\n",
      "train loss 0.576 valid loss 0.833\n",
      "train loss 0.567 valid loss 0.840\n",
      "train loss 0.563 valid loss 0.848\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=15, lr=0.01, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.562 valid loss 0.841\n",
      "train loss 0.555 valid loss 0.836\n",
      "train loss 0.550 valid loss 0.831\n",
      "train loss 0.545 valid loss 0.828\n",
      "train loss 0.541 valid loss 0.826\n",
      "train loss 0.538 valid loss 0.824\n",
      "train loss 0.535 valid loss 0.823\n",
      "train loss 0.533 valid loss 0.821\n",
      "train loss 0.530 valid loss 0.820\n",
      "train loss 0.528 valid loss 0.820\n",
      "train loss 0.526 valid loss 0.819\n",
      "train loss 0.524 valid loss 0.818\n",
      "train loss 0.522 valid loss 0.818\n",
      "train loss 0.519 valid loss 0.817\n",
      "train loss 0.517 valid loss 0.817\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=15, lr=0.001, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.515 valid loss 0.817\n",
      "train loss 0.512 valid loss 0.817\n",
      "train loss 0.509 valid loss 0.818\n",
      "train loss 0.507 valid loss 0.818\n",
      "train loss 0.504 valid loss 0.818\n",
      "train loss 0.502 valid loss 0.818\n",
      "train loss 0.499 valid loss 0.819\n",
      "train loss 0.496 valid loss 0.819\n",
      "train loss 0.494 valid loss 0.819\n",
      "train loss 0.491 valid loss 0.819\n",
      "train loss 0.488 valid loss 0.819\n",
      "train loss 0.486 valid loss 0.819\n",
      "train loss 0.483 valid loss 0.819\n",
      "train loss 0.480 valid loss 0.820\n",
      "train loss 0.477 valid loss 0.820\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=15, lr=0.001, wd=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MF with bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF_bias(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size=100):\n",
    "        super(MF_bias, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        # init \n",
    "        self.user_emb.weight.data.uniform_(0,0.05)\n",
    "        self.item_emb.weight.data.uniform_(0,0.05)\n",
    "        self.user_bias.weight.data.uniform_(-0.01,0.01)\n",
    "        self.item_bias.weight.data.uniform_(-0.01,0.01)\n",
    "        \n",
    "    def forward(self, u, v):\n",
    "        U = self.user_emb(u)\n",
    "        V = self.item_emb(v)\n",
    "        b_u = self.user_bias(u).squeeze()\n",
    "        b_v = self.item_bias(v).squeeze()\n",
    "        return (U*V).sum(1) +  b_u  + b_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MF_bias(num_users, num_items, emb_size=100) #.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 13.232 valid loss 4.428\n",
      "train loss 4.375 valid loss 3.504\n",
      "train loss 3.483 valid loss 2.645\n",
      "train loss 2.469 valid loss 0.994\n",
      "train loss 0.787 valid loss 2.035\n",
      "train loss 1.813 valid loss 2.752\n",
      "train loss 2.520 valid loss 2.389\n",
      "train loss 2.139 valid loss 1.547\n",
      "train loss 1.274 valid loss 1.187\n",
      "train loss 0.904 valid loss 1.537\n",
      "train loss 1.283 valid loss 1.534\n",
      "train loss 1.344 valid loss 1.059\n",
      "train loss 0.932 valid loss 0.909\n",
      "train loss 0.815 valid loss 1.135\n",
      "train loss 1.044 valid loss 1.315\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=15, lr=0.1, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.209 valid loss 1.013\n",
      "train loss 0.894 valid loss 0.859\n",
      "train loss 0.721 valid loss 0.835\n",
      "train loss 0.675 valid loss 0.880\n",
      "train loss 0.698 valid loss 0.924\n",
      "train loss 0.722 valid loss 0.936\n",
      "train loss 0.718 valid loss 0.922\n",
      "train loss 0.692 valid loss 0.902\n",
      "train loss 0.664 valid loss 0.887\n",
      "train loss 0.646 valid loss 0.881\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.01, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.640 valid loss 0.873\n",
      "train loss 0.633 valid loss 0.865\n",
      "train loss 0.627 valid loss 0.858\n",
      "train loss 0.622 valid loss 0.852\n",
      "train loss 0.617 valid loss 0.846\n",
      "train loss 0.613 valid loss 0.841\n",
      "train loss 0.609 valid loss 0.837\n",
      "train loss 0.606 valid loss 0.833\n",
      "train loss 0.603 valid loss 0.829\n",
      "train loss 0.600 valid loss 0.826\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.001, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.597 valid loss 0.824\n",
      "train loss 0.595 valid loss 0.822\n",
      "train loss 0.592 valid loss 0.820\n",
      "train loss 0.590 valid loss 0.818\n",
      "train loss 0.587 valid loss 0.817\n",
      "train loss 0.585 valid loss 0.816\n",
      "train loss 0.583 valid loss 0.815\n",
      "train loss 0.580 valid loss 0.814\n",
      "train loss 0.578 valid loss 0.813\n",
      "train loss 0.576 valid loss 0.812\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.001, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.574 valid loss 0.812\n",
      "train loss 0.571 valid loss 0.811\n",
      "train loss 0.569 valid loss 0.811\n",
      "train loss 0.567 valid loss 0.811\n",
      "train loss 0.564 valid loss 0.811\n",
      "train loss 0.562 valid loss 0.811\n",
      "train loss 0.560 valid loss 0.810\n",
      "train loss 0.557 valid loss 0.810\n",
      "train loss 0.555 valid loss 0.810\n",
      "train loss 0.552 valid loss 0.810\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.001, wd=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these models are susceptible to weight initialization, optimization algorithm and regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note here there is no matrix multiplication, we could potentially make the embeddings \n",
    "# for users and items of different sizes.\n",
    "# Here we could get better results by keep playing with regularization.\n",
    "    \n",
    "class CollabFNet(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size=100, n_hidden=20):\n",
    "        super(CollabFNet, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        self.lin1 = nn.Linear(emb_size*2, n_hidden)\n",
    "        self.lin2 = nn.Linear(n_hidden, 1)\n",
    "        self.drop1 = nn.Dropout(0.3)\n",
    "        self.drop2 = nn.Dropout(0.0)\n",
    "        self.dense_bn = nn.BatchNorm1d(n_hidden)\n",
    "        \n",
    "    def forward(self, u, v):\n",
    "        U = self.user_emb(u)\n",
    "        V = self.item_emb(v)\n",
    "        x = torch.cat([U, V], dim=1)\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.dense_bn(self.lin1(x)))\n",
    "        x = self.drop2(x)\n",
    "        x = self.lin2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CollabFNet(num_users, num_items, emb_size=120, n_hidden=40) #.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 12.418 valid loss 5.096\n",
      "train loss 5.841 valid loss 7.370\n",
      "train loss 1.697 valid loss 26.980\n",
      "train loss 2.557 valid loss 30.591\n",
      "train loss 3.289 valid loss 19.496\n",
      "train loss 1.930 valid loss 9.190\n",
      "train loss 1.028 valid loss 3.707\n",
      "train loss 1.077 valid loss 1.729\n",
      "train loss 1.468 valid loss 1.230\n",
      "train loss 1.689 valid loss 1.168\n",
      "train loss 1.604 valid loss 1.256\n",
      "train loss 1.303 valid loss 1.509\n",
      "train loss 0.989 valid loss 1.959\n",
      "train loss 0.831 valid loss 2.498\n",
      "train loss 0.889 valid loss 2.838\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=15, lr=0.1, wd=1e-5, unsqueeze=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.044 valid loss 1.842\n",
      "train loss 0.861 valid loss 1.287\n",
      "train loss 0.770 valid loss 1.005\n",
      "train loss 0.736 valid loss 0.889\n",
      "train loss 0.743 valid loss 0.863\n",
      "train loss 0.761 valid loss 0.874\n",
      "train loss 0.778 valid loss 0.891\n",
      "train loss 0.780 valid loss 0.902\n",
      "train loss 0.770 valid loss 0.902\n",
      "train loss 0.754 valid loss 0.893\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.01, wd=1e-5, unsqueeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.736 valid loss 0.848\n",
      "train loss 0.699 valid loss 0.841\n",
      "train loss 0.704 valid loss 0.837\n",
      "train loss 0.703 valid loss 0.837\n",
      "train loss 0.692 valid loss 0.851\n",
      "train loss 0.680 valid loss 0.875\n",
      "train loss 0.679 valid loss 0.897\n",
      "train loss 0.678 valid loss 0.907\n",
      "train loss 0.675 valid loss 0.904\n",
      "train loss 0.668 valid loss 0.892\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.01, wd=1e-5, unsqueeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.663 valid loss 0.881\n",
      "train loss 0.659 valid loss 0.873\n",
      "train loss 0.660 valid loss 0.867\n",
      "train loss 0.661 valid loss 0.864\n",
      "train loss 0.658 valid loss 0.863\n",
      "train loss 0.654 valid loss 0.863\n",
      "train loss 0.656 valid loss 0.863\n",
      "train loss 0.657 valid loss 0.863\n",
      "train loss 0.657 valid loss 0.863\n",
      "train loss 0.654 valid loss 0.863\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.001, wd=1e-5, unsqueeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epocs(model, epochs=10, lr=0.001, wd=1e-5, unsqueeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epocs(model, epochs=10, lr=0.001, wd=1e-5, unsqueeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epocs(model, epochs=10, lr=0.001, wd=1e-5, unsqueeze=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "* use t-sne to visualize embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab\n",
    "* Can you use `tags.csv` and `timestamp` to improve your predictions?\n",
    "* Play with the hyperparameters\n",
    "* Look at fastai version of this network and try his transformation https://github.com/fastai/fastai/blob/master/courses/dl1/lesson5-movielens.ipynb\n",
    "* You may need a dataloader if you data is larger. Can you construct a dataset? Here is an example:\n",
    "https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel.html\n",
    "* Work with the largest dataset http://files.grouplens.org/datasets/movielens/ml-latest.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "* This notebook is based on [lesson 5 of Jeremy Howard's Deep Learning Course](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson5-movielens.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
